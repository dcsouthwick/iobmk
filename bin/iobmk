#!/usr/bin/env python3
"""
###############################################################################
# Copyright 2019-2021 CERN. See the COPYRIGHT file at the top-level directory
# of this distribution. For licensing information, see the COPYING file at
# the top-level directory of this distribution.
################################################################################
"""

import argparse
import datetime
import logging
import sys
import socket
import os
import textwrap
import time
import yaml

from iobenchmarksuite.iobenchmarksuite import IOBenchmarkSuite

from iobenchmarksuite import utils
from iobenchmarksuite import config

from iobenchmarksuite.exceptions import PreFlightError
from iobenchmarksuite.exceptions import BenchmarkFailure
from iobenchmarksuite.exceptions import BenchmarkFullFailure

from iobenchmarksuite.plugins import send_queue
from iobenchmarksuite.__version__ import __version__


class Color:
    """Console colors."""

    CYAN = "\033[96m"
    GREEN = "\033[92m"
    YELLOW = "\033[93m"
    RED = "\033[91m"
    BOLD = "\033[1m"
    END = "\033[0m"
    WHITE = "\033[97m"


def main():
    """bmkrun cli."""
    parser = argparse.ArgumentParser(
        prog="bmkrun",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=textwrap.dedent(
            """
        -----------------------------------------------
        IO Benchmark Suite
        -----------------------------------------------
        This utility orchestrates several benchmarks.

        Author: Benchmarking Working Group
        Contact: benchmark-suite-wg-devel@cern.ch
        """
        ),
        epilog=textwrap.dedent(
            """
        -----------------------------------------------
        """
        ),
    )

    # Arguments
    parser.add_argument(
        "-b", "--benchmarks", nargs="+", help="List of benchmarks.", default=None
    )

    parser.add_argument(
        "-c",
        "--config",
        nargs="?",
        type=str,
        help="REQUIRED - Configuration file to use (yaml format). Accepts '-c default'.",
        default=None,
    )

    parser.add_argument(
        "-d",
        "--rundir",
        nargs="?",
        help="Directory where benchmarks will be run.",
        default=None,
    )

    parser.add_argument(
        "-e",
        "--export",
        action="store_true",
        help="Export all json and log files from rundir and compresses them.",
        default=None,
    )

    parser.add_argument(
        "-m",
        "--mode",
        choices=["singularity", "docker"],
        nargs="?",
        help="Run benchmarks in singularity or docker containers.",
        default=None,
    )

    parser.add_argument(
        "-n",
        "--mp_num",
        nargs="?",
        type=int,
        help="Number of cpus to run the benchmarks.",
        default=None,
    )

    parser.add_argument(
        "-p",
        "--publish",
        action="store_true",
        help="Enable reporting via AMQ credentials in YAML file.",
        default=None,
    )

    parser.add_argument(
        "-s",
        "--show",
        action="store_true",
        help="Show running config and exit.",
        default=None,
    )

    parser.add_argument(
        "-t",
        "--tags",
        action="store_true",
        help="Enable reading of user tags from ENV variables (BMKSUITE_TAG_{TAG}). Tags specified in configuration file are ignored.",
        default=None,
    )

    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="Enable verbose mode. Display debug messages.",
        default=None,
    )

    parser.add_argument(
        "--version", action="version", version="{version}".format(version=__version__)
    )

    args = parser.parse_args()

    # Select the config file to load
    # load default configuration shipped with IO benchmark suite
    if args.config == "default":
        load_config = os.path.join(config.__path__[0], "benchmarks.yml")

    # No configuration file was provided
    elif args.config is None:
        parser.print_help()
        print("{}No configuration file specified.{}".format(Color.RED, Color.END))
        print(
            "{}Please specify a configuration or run with the default: bmkrun -c default {}".format(
                Color.RED, Color.END
            )
        )
        sys.exit(1)

    else:
        load_config = args.config

    # Load configuration file
    try:
        with open(load_config, "r") as yam:
            active_config = yaml.full_load(yam)

    except FileNotFoundError:
        print(
            "{0}Failed to load configuration file: {1} {2}".format(
                Color.RED, load_config, Color.END
            )
        )
        sys.exit(1)

    # Check for cli overrides
    # Convert arguments to dict
    temp_config = vars(args)
    del temp_config["config"]

    # Get non-None cli arguments to override config file
    non_empty = {k: v for k, v in temp_config.items() if v is not None}

    # Populate active config with cli override
    for i in non_empty.keys():
        if i == "tags":
            # Update tags with json format
            active_config["global"]["tags"] = utils.get_tags_env()
        else:
            active_config["global"][i] = non_empty[i]

    # Check if user provided a benchmark
    if active_config["global"]["benchmarks"] is None:
        parser.print_help()
        print("{}No benchmarks were selected. {}".format(Color.YELLOW, Color.END))
        sys.exit(1)

    # Check if user provided valid benchmark
    AVAILABLE_BENCHMARKS = ("db12", "hepscore", "spec2017", "hs06")

    for bench in active_config["global"]["benchmarks"]:
        if bench not in AVAILABLE_BENCHMARKS:
            print(
                '{}Benchmark "{}" is not a valid benchmark.{}'.format(
                    Color.RED, bench, Color.END
                )
            )
            print(
                "Please select one of the following benchmarks:\n- {}".format(
                    "\n- ".join(AVAILABLE_BENCHMARKS)
                )
            )
            sys.exit(1)

    # use all CPUs found if invalid parameter provided
    if (
        "mp_num" not in active_config["global"].keys()
        or active_config["global"]["mp_num"] is None
        or int(active_config["global"]["mp_num"]) > os.cpu_count()
    ):
        active_config["global"]["mp_num"] = os.cpu_count()

    # Check if cpu count in config is integer
    if not isinstance(active_config["global"]["mp_num"], int):
        print("{}CPU number (mp_num) is not an integer.{}".format(Color.RED, Color.END))
        sys.exit(1)

    print("# The following configuration was loaded: {}".format(load_config))
    if len(non_empty):
        print(
            "# The configuration was overridden by the following CLI args: {}".format(
                non_empty
            )
        )

    # Print running configuration and exit
    if args.show:
        print(yaml.dump(active_config))
        sys.exit(0)

    # Create another dict key to mark the user specified rundir as parent_dir
    # Append the date to the rundir in order to group the results per date
    # Create parent_dir, example: /tmp/io-benchmark-suite
    active_config["global"]["parent_dir"] = active_config["global"]["rundir"]
    os.makedirs(active_config["global"]["parent_dir"], exist_ok=True)

    # Create rundir, example: /tmp/io-benchmark-suite/run_date
    active_config["global"]["rundir"] = os.path.join(
        active_config["global"]["rundir"],
        "run_{}".format(time.strftime("%Y-%m-%d_%H%M", time.gmtime())),
    )

    os.makedirs(active_config["global"]["rundir"], exist_ok=True)

    # Configure logging
    # Log verbosity
    if args.verbose:
        log_level = logging.DEBUG
    else:
        log_level = logging.INFO

    # Enable logging
    logger = logging.getLogger()
    logger.setLevel(log_level)

    # Log format
    log_formatter = logging.Formatter(
        "%(asctime)s, %(name)s:%(funcName)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # Handler to write logs to stdout
    stream_handler = logging.StreamHandler(sys.stdout)
    stream_handler.setFormatter(log_formatter)
    stream_handler.setLevel(log_level)

    # Handler to write logs to file
    LOG_PATH = os.path.join(active_config["global"]["rundir"], "io-benchmark-suite.log")
    file_handler = logging.FileHandler(LOG_PATH)
    file_handler.setFormatter(log_formatter)
    file_handler.setLevel(log_level)

    # Select loggers
    logger.addHandler(stream_handler)
    logger.addHandler(file_handler)

    # Save running config
    with open(
        os.path.join(active_config["global"]["rundir"], "run_config.yaml"), "w"
    ) as conf_file:
        yaml.dump(active_config, conf_file)

    # Configure io-benchmark-suite
    logger.debug("Active configuration in use: %s", active_config)
    suite = IOBenchmarkSuite(config=active_config)

    try:
        suite.start()
    except (PreFlightError, BenchmarkFullFailure):
        logger.error("HEP-Benchmark Suite failed.")
        sys.exit(1)
    except BenchmarkFailure:
        logger.warning(
            "HEP-Benchmark Suite ran with failed benchmarks. Please be aware of the results."
        )

    # Export logs and json to a compressed tarball
    # format of export: dirname_hostname_datetime.tar.gz
    if args.export:
        utils.export(
            active_config["global"]["rundir"],
            "{}_{}_{}.tar.gz".format(
                os.path.split(active_config["global"]["rundir"])[-1],
                socket.gethostname(),
                datetime.datetime.now().strftime("%Y-%m-%d_%H%M"),
            ),
        )

    # Display results
    FULL_PATH = os.path.join(active_config["global"]["rundir"], "bmkrun_report.json")
    utils.print_results_from_file(FULL_PATH)

    # Publish to AMQ broker if provided
    if active_config["global"].get("publish"):
        try:
            send_queue.send_message(FULL_PATH, active_config["activemq"])
        except Exception as err:
            logger.error("Something went wrong attempting to report via AMQ.")
            logger.error("Results may not have been correctly transmitted.")
            logger.exception(err)

    print(
        "\n{}Full results can be found in {} {}".format(
            Color.CYAN, FULL_PATH, Color.END
        )
    )
    print(
        "{}Full run log can can be found in {} {}".format(
            Color.CYAN, LOG_PATH, Color.END
        )
    )


if __name__ == "__main__":
    main()
