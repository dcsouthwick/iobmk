#!/usr/bin/env python3

###############################################################################
## Copyright 2019-2020 CERN. See the COPYRIGHT file at the top-level directory
## of this distribution. For licensing information, see the COPYING file at
## the top-level directory of this distribution.
################################################################################

import argparse
import logging
import sys
import os
import yaml
import urllib.request
try:
    import importlib.resources as pkg_resources
except ImportError:
    # Try backported to PY<37 `importlib_resources`.
    import importlib_resources as pkg_resources

from hepbenchmarksuite.hepbenchmarksuite import HepBenchmarkSuite
from hepbenchmarksuite import utils
from hepbenchmarksuite import config

def main():
    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter,
                                description="-"*70+"\n bmkrun cli allows you to run several benchmarks\n"+"-"*70+"\nAuthor: Benchmarking Working Group\nContact: benchmark-suite-wg-devel@cern.ch", epilog="-"*70)
    
    # may be a better way to do this...
    with pkg_resources.path(config, 'benchmarks.yml') as default_config_path:
      default_config = default_config_path
    # Optional arguments
    parser.add_argument("-b", "--benchmarks", nargs='+', help="List of benchmarks", default=None)
    parser.add_argument("-c", "--config",     nargs='?', type=str, help="Configuration file to use (yaml format)", default=default_config)
    parser.add_argument("-d", "--rundir",     nargs='?', help="Directory where benchmarks will be run",  default=None)
    parser.add_argument("-f", "--file",       nargs='?', help="File to store the results", default=None)
    parser.add_argument("-m", "--mode",       choices=['singularity', 'docker'], nargs='?', help="Run benchmarks in singularity or docker containers", default=None)
    parser.add_argument("-n", "--mp_num",     nargs='?', type=int, help="Number of cpus to run the benchmarks", default=None)
    parser.add_argument("-t", "--tags",       nargs='?', help="Custom user tags", default=None)
    parser.add_argument("-u", "--uid",        nargs='?', help="UID", default=None)
    parser.add_argument("-v", "--verbose",    action='store_true', help="Enables verbose mode. Display debug messages.", default=None)

    try:
      args = parser.parse_args()
    except:
      sys.exit(0)

    # Log verbosity
    if args.verbose:
       log_level = logging.DEBUG
    else:
       log_level = logging.INFO

    # Enable logging
    logging.basicConfig(stream=sys.stdout, level=log_level)

    # Select config file
    if args.config != None:
      with open(args.config, 'r') as yam:
        conf_data = yaml.full_load(yam)

      active_config = conf_data
    else:
      parser.print_help()
      sys.exit("No config file specified.")

    # Check for cli overrides
    # Convert arguments to dict
    temp_config = vars(args)
    del temp_config['config']

    # Get non-None cli arguments to override config file
    non_empty = {k: v for k, v in temp_config.items() if v is not None}

    # Populate active config with cli override
    for i in non_empty.keys():
      if i == 'tags':
        # Update tags with json format
        active_config['global']['tags'] = utils.convert_tags_to_json(args.tags)
      else:
        active_config['global'][i] = non_empty[i]

    # TO BE IMPROVED: Run config validation

    # Check if user provided a benchmark
    if active_config['global']['benchmarks'] == None:
      parser.print_help()
      sys.exit("No benchmarks were provided.")

    # Check if user provided valid benchmark
    AVAILABLE_BENCHMARKS  = ["db12" , "hepscore", "spec2017", "hs06_32", "hs06_64", "test"]
    for bench in active_config['global']['benchmarks']:
      if bench not in AVAILABLE_BENCHMARKS:
        sys.exit('Benchmark "{}" is not a valid benchmark. Please select one of the following benchmarks:\n- {} '.format(bench,'\n- '.join(AVAILABLE_BENCHMARKS)))

    # Add CPU count if not present
    if 'mp_num' not in active_config['global'].keys():
      active_config['global']['mp_num'] = os.cpu_count()

    # Check for a valid CPU count
    if active_config['global']['mp_num'] == None or (int(active_config['global']['mp_num']) > os.cpu_count()):
      active_config['global']['mp_num'] = os.cpu_count()

    # Check if cpu count in config is integer
    if not isinstance(active_config['global']['mp_num'], int):
      sys.exit("CPU number is not an integer.")

    # All config checks passed
    # Dump running config
    try:
      os.makedirs(active_config['global']['rundir'])
    except OSError as e:
      logging.info("Skiping directory creation: {}".format(e))

    with open(os.path.join(active_config['global']['rundir'], 'run_config.yaml'), 'w') as conf_file:
      yaml.dump(active_config, conf_file)

    # Configure hep-benchmark-suite
    logging.debug("Active configuration in use: {}".format(active_config))
    suite = HepBenchmarkSuite(config=active_config)
    suite.start()

    # Display results
    FULL_PATH  = os.path.join(active_config['global']['rundir'], active_config['global']['file'])
    utils.print_results_from_file(FULL_PATH)

    print("\nFull results can be found in {}".format(FULL_PATH))

if __name__ == "__main__":
    main()

